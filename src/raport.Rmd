---
output:
  pdf_document: default
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dendextend)
library(mclust)
library(ggplot2)
library(gridExtra)
set.seed(100)
    
setsPath <- "D:/Studia/_PrzetwarzanieDanych/praca_domowa2/zbiory-benchmarkowe";

    Compare_hclust_methods <- function(fileName) {
        setwd(setsPath);
        data <- read.table(paste0(fileName, ".data.gz"));
        labels <- as.integer(read.table(paste0(fileName, ".labels0.gz"))[,1]);
        k <- max(labels);
        
        # Process algorithms
        resultsFolder <- paste0(setsPath, "/results/hclust_")
        hCompleteResult <- read.table(paste0(resultsFolder, "complete", "/", fileName, ".csv"));
        hSingleResult <- read.table(paste0(resultsFolder, "single", "/", fileName, ".csv"));
        hAverageResult <- read.table(paste0(resultsFolder, "average", "/", fileName, ".csv"));
        hWardResult <- read.table(paste0(resultsFolder, "ward.D2", "/", fileName, ".csv"));
        hMcquittyResult <- read.table(paste0(resultsFolder, "mcquitty", "/", fileName, ".csv"));
        hMedianResult <- read.table(paste0(resultsFolder, "median", "/", fileName, ".csv"));
        hCentroidResult <- read.table(paste0(resultsFolder, "centroid", "/", fileName, ".csv"));
         
        # Find similarity indices
        dt <- data.frame("Method" = "Complete", "Index" = "FM", "Value" = FM_index(hCompleteResult, labels)[1], stringsAsFactors = FALSE);
        dt[nrow(dt) + 1,] = list("Complete", "AR", adjustedRandIndex(hCompleteResult, labels));
        dt[nrow(dt) + 1,] = list("Single", "FM", FM_index(hSingleResult, labels)[1]);
        dt[nrow(dt) + 1,] = list("Single", "AR", adjustedRandIndex(hSingleResult, labels));
        dt[nrow(dt) + 1,] = list("Average", "FM", FM_index(hAverageResult, labels)[1]);
        dt[nrow(dt) + 1,] = list("Average", "AR", adjustedRandIndex(hAverageResult, labels));
        dt[nrow(dt) + 1,] = list("ward.D2", "FM", FM_index(hWardResult, labels)[1]);
        dt[nrow(dt) + 1,] = list("ward.D2", "AR", adjustedRandIndex(hWardResult, labels));
        dt[nrow(dt) + 1,] = list("mcquitty", "FM", FM_index(hMcquittyResult, labels)[1]);
        dt[nrow(dt) + 1,] = list("mcquitty", "AR", adjustedRandIndex(hMcquittyResult, labels));
        dt[nrow(dt) + 1,] = list("median", "FM", FM_index(hMedianResult, labels)[1]);
        dt[nrow(dt) + 1,] = list("median", "AR", adjustedRandIndex(hMedianResult, labels));
        dt[nrow(dt) + 1,] = list("centroid", "FM", FM_index(hCentroidResult, labels)[1]);
        dt[nrow(dt) + 1,] = list("centroid", "AR", adjustedRandIndex(hCentroidResult, labels));
        return(dt);
    }


Compare_method <- function(fileName, methodName) {
        setwd(setsPath);
        labels <- as.integer(read.table(paste0(fileName, ".labels0.gz"))[,1]);
        
        result <- read.table(paste0(setsPath, "/results/", methodName, "/", fileName, ".csv"));
        
        dt <- data.frame("Set name" = fileName, "Index" = "FM", "Value" = FM_index(result, labels)[1], stringsAsFactors = FALSE);
        dt[nrow(dt) + 1,] = list(fileName, "AR", abs(adjustedRandIndex(result, labels)));
        
        return(dt)
}

plotComparedMethod <- function(df, title) {
    meanIndex <- mean(aggregate(df$Value, by=list(df$Set.name), FUN=sum)[[2]]);
    subtitle <- paste0("avg(FM + AR) = ", toString(round(meanIndex, digits = 4)))
    
    ggplot(df, aes(factor(Set.name), Value, fill = Index)) + 
        geom_bar(stat="identity", position = "dodge") +  
        scale_fill_brewer(palette = "Set1") + 
        theme(axis.ticks.x=element_blank(), axis.text.x=element_blank()) +
        xlab("Zbiór") + ylab("Wartość wskaźnika") +
        ggtitle(title) + labs(subtitle=subtitle)
}

drawComparisonPlot <- function(resultsFolder, plotTitle) {
    results <- lapply(fileNames, Compare_method, methodName=resultsFolder);
    df <- Reduce(rbind, results);
    plotComparedMethod(df, plotTitle);
}

setwd(setsPath);
fileNames <- list.files()[grepl(pattern = ".data.gz", list.files())];
    for(i in 1:length(fileNames)) {
        fileNames[i] <- substr(fileNames[i], 1, nchar(fileNames[i]) - 8);
    }
```

# Wstęp
Analiza skupień jest zadaniem eksploracji i etykietowania danych, które polega na dzieleniu zbioru danych na grupy (skupienia) tak, by elementy w jednej grupie były do siebie jak najbardziej podobne, a jednocześnie jak najbardziej odmienne od elementów z pozostałych grup. Jako że analiza skupień znalazła swoje zastosowanie w wielu dziedzinach nauki, dostępna jest mnogość bibliotek, które implementują ten algorytm. Zajmiemy się poniższymi implementacjami:

* algorytmy hierarchiczne `hlcust` dostępne w bazowym R,
* algorytmem genie z pakietu Genie,
* algorytmem hierarchicznym z pakietu `protoclust`,
* autorską implementacją analizy skupień.

Interesujące może się wydawać porównanie dostępnych implementacji i wybranie tej najlepszej. Czy jest to jednak wybór oczywisty? Czy wydajność i dokładność algorytmów zależy od zbioru danych? W odpowiedzi na te pytania pomogą wskaźniki FM i AR, które jednoznacznie porównają wyniki zwracane przez algorytmy dla zbiorów benchmarkowych z wzorcowymi. Zarówno **skorygowany indeks Randa**, jak i **indeks Fowlkesa–Mallowsa** pozwalają ocenić zgodność dwóch podziałów zbioru na rozłączne podzbiory. Ponadto, AR uwzględnia prawdopodobieństwo, że dwa algorytmy grupowania zachowując się losowo, równocześnie rozdzielą parę lub dołączą do jednej grupy.


# Własna implementacja analizy skupień
```r
function(X, k, M) {
    S <- Mnn(X, M);
    G <- Mnn_graph(S);
    D <- Mnn_graph_D_matrix(G);
    G <- Mnn_connect_graph(G);
    E <- Laplacian_eigen(D, G, k);
    set.seed(100);
    return(kmeans(E, k));
}
```
Autorska implementacja algorytmu jest pewnego rodzaju heurystyką - na początku ma na celu znormalizowanie danych, a potem zasotowanie bazowego algorytmu k-średnich. Normalizacja jest oparta na trzech najważniejszych krokach:

1. **Wyznaczenie macierzy najbliższych sąsiadów.**

Na początku algorytm wyznacza macierz najbliższych sąsiadów, wywołując `Mnn`. Kod tej funkcji jest napisany w języku C++. Wyznaczanie najbliższych sąsiadów odbywa się z wykorzystaniem `priority_queue` dla lepszej złożoności zamortyzowanej.

2. **Wyznaczenie macierzy sąsiedztwa.**

Następnie funkcja `Mnn_graph` generuje symetryczną macierz sąsiedztwa dla pewnego grafu. Potem, funkcja `Mnn_graph_D_matrix` tworzy macierz diagonalną reprezentującą stopnie w grafie. Po stworzeniu macierzy, wywołanie `Mnn_connect_graph` łączy spójne składowe grafu i zapisuje te zmiany do macierzy sąsziedztwa. Spójne składowe są wyznaczane, przechodząc po grafie algorytmem przeszukiwania w głąb (DFS). Łączone są wierzchołki wyznaczone jako pierwsze dla każdej składowej podczas przeszukiwania.

3. **Wyznaczanie laplasjanu i jego wektorów własnych.**

Kolejnym etapem algorytmu jest wyznaczenie laplasjanu grafu, którego składowe jeszcze nie zostały połączone. Finalnie, wyznaczane są wartości własne wyznaczonego laplasjanu. Wyznacznie ich przy użyciu bazowej funkcji `base::eigen` było bardzo nieefektywne, nawet przy poinformowaniu funkcji o symetryczności macierzy. Zamiast tego wykorzystana została funkcja `RSpectra::eigs_sym`, która wykonywała się znacząco szybciej dla wszystkich zbiorów benchmarkowych. Wartym zaznaczenia jest fakt, że to właśnie wyznaczanie wartości własnych jest najbardziej czasochłonnym krokiem algorytmu.

\newpage

# Indeksy FM i AR dla bazowych algorytmów hierarchicznych

Ogólnie, hierarchiczne algorytmy analizy skupień zwracają rodzinę zagnieżdżonych podziałów taką, że każdy jej element jest sam w sobie poprawnym grupowaniem. Takie metody oferują więc nie tylko wgląd w strukturę zbioru danych, ale także możliwość obserwowania procesu formowania skupień. Jakość algorytmów z bazowego R (`base::hclust`) jest zobrazowana na poniższych wykresach, dla 3 przykładowych zbiorów benchmarkowych.

```{r echo=FALSE, fig.height=7, fig.align='center'}
    benchmarkSets <- c("cross", "dense", "lsun")
    hclustResult <- lapply(benchmarkSets, Compare_hclust_methods)
    
    getPlot <- function(hclustResult, index, benchmarkSets) {
        return(ggplot(hclustResult[[index]], aes(factor(Method), Value, fill = Index)) + 
        geom_bar(stat="identity", position = "dodge") +  
        ggtitle(paste(benchmarkSets[index])) +
        scale_fill_brewer(palette = "Set1") + 
        theme(axis.text.x=element_text(angle=20, hjust=1), axis.title.x=element_blank()) +
        geom_text(aes(label = round(Value, digits = 2)), position=position_dodge(width=0.9), vjust=-0.25, size=2) + ylab("Wartość wskaźnika"))
    }
    
    p1 <- getPlot(hclustResult, 1, benchmarkSets)
    p2 <- getPlot(hclustResult, 2, benchmarkSets)
    p3 <- getPlot(hclustResult, 3, benchmarkSets)
    
    grid.arrange(p1, p2, p3, nrow=3)
```

Jak widać, średnio bazowe algorytmy hierarchiczne nie dają bardzo dokładnych wyników. Jednak wybierając odpowiednią metodę w zależności od zbioru danych, można uzyskać zadowalającą dokładność. 

\newpage

# Indeksy FM i AR dla algorytmu Genie

```{r echo=FALSE, fig.height=3.4, fig.align='center'}
    drawComparisonPlot("genie", "Genie");
```
Funkcja `hclust2` z pakietu Genie jest hierarchicznym algorytmem analizy skupień opartą na minimalnym drzewie rozpinającym. Algorytm wypadł najlepiej zarówno czasowo, jak i ze względu na dokładność działania. Szybkość algorytmu ma swoje uzasadnienie w zrównolegleniu i wprowadzeniu nowego kryterium łączenia. Złożoność czasowa to $O(n^2)$, a pamięciowa - $O(n)$.

\newpage

# Indeksy FM i AR dla pakietu protoclust
Pakiet `protoclust`, dostępny na [CRAN](https://cran.r-project.org/web/packages/protoclust/index.html), udostępnia funkcję `protoclust`, która dokonuje analizy skupień na zadanym zbiorze danych. Zgodnie ze specyfikacją, algorytm jest również algorytmem hierarchicznym.

> Performs minimax linkage hierarchical clustering given a set of dissimilarities. Consider two clusters G and H and their union U. The minimax linkage between G and H is defined to be the radius of the smallest ball that encloses all of U and that is centered at one of the points in U.

Algorytm został zaimplementowany w 2011 przez Jacob Bien i Robert Tibshirani. Bazuje on na opisie algorytmu z "A Survey of Recent Advances in Hierarchical Clustering Algorithms" - publikacji z 1983 roku. Ta implementacja, podobnie do autorskiej, wykorzystuje algorytm najbliższych sąsiadów.

Poniżej zestawienie wyników działania algorytmu dla przykładowych zbiorów benchmarkowych.

```{r fig.width=6, fig.height=3.4, echo=FALSE, fig.align='center'}
    drawComparisonPlot("protoclust", "protoclust");
```

\newpage

# Indeksy FM i AR dla własnej implementacji algorytmu 

W przypadku autorskiej heurystyki, bardzo istotnym parametrem wpływającym na wynik jest M w algorytmie najbliższych sąsiadów. Poniżej wykresy reprezentujące indeksy dla zbiorów benchmarkowych dla różnych wartości M:

```{r fig.width=6, fig.height=3.4, echo=FALSE, fig.align='center'}
    drawComparisonPlot("custom_5", "M = 5")
```
```{r fig.width=6, fig.height=3.4, echo=FALSE, fig.align='center'}
    drawComparisonPlot("custom_20", "M = 20")
```
```{r fig.width=6, fig.height=3.4, echo=FALSE, fig.align='center'}
     drawComparisonPlot("custom_100", "M = 100")
```
```{r fig.width=6, fig.height=3.4, echo=FALSE, fig.align='center'}
    drawComparisonPlot("custom_200", "M = 200")
```
```{r fig.width=6, fig.height=3.4, echo=FALSE, fig.align='center'}
    drawComparisonPlot("custom_500", "M = 500")
```

Analizując średnią sume indeksów FM i AR, można dojść do wniosku, że zwiększanie M w algorytmie najbliższych sąsiadów jedynie pogarsza wyniki heurystyki.
